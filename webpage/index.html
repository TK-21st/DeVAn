<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta
      name="description"
      content="DeVAn: Dense Video Annotation for Video-Language Models"
    />
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta
      property="og:description"
      content="SOCIAL MEDIA DESCRIPTION TAG TAG"
    />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="images/compare_benchmark.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />

    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG" />
    <meta
      name="twitter:description"
      content="TWITTER BANNER DESCRIPTION META TAG"
    />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="images/compare_benchmark.png" />
    <meta name="twitter:card" content="summary_large_image" />
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>
      DeVAn: Dense Video Annotation for Video-Language Models
    </title>
    <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="static/css/index.css" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script src="https://code.jquery.com/jquery-3.7.0.js"></script>
    <script src="https://cdn.datatables.net/1.13.7/js/jquery.dataTables.min.js"></script>
    <link
      href="https://cdn.datatables.net/1.13.7/css/jquery.dataTables.min.css"
      rel="stylesheet"
    />

    <link rel="icon" href="images/InfiMM_team_logo_original.png" />

    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
  </head>
  <body>
    <!-- <div class="container">
      <div class="column has-text-centered">
        <img
          style="max-width: 200px; margin-bottom: -50px"
          src="images/InfiMM-Eval-logo.png"
        />
      </div>
    </div> -->

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                DeVAn: Dense Video Annotation for Video-Language Models
              </h1>
              <div class="is-size-5 publication-authors">
                <!-- Paper authors -->
                <span class="author-block">
                  <!-- <img
                    src="images/InfiMM_team_logo_original.png"
                    style="width: 1em; vertical-align: middle"
                    alt="Logo"
                  /> -->
                  <span class="mathvista">Anonymous Authors</span>
                  <!-- <sup style="color: #6fbf73">1</sup>
                  <sup style="color: #ed4b82">2</sup>
                  <sup style="color: #ffac33">3</sup> -->
                </span>

                <!-- <span class="author-block">
              InfiMM Team @ University of Texas at Austin, Institute of Automation, Chinese Academy of Sciences and ByteDance Ltd.
            </span> -->
              </div>

              <!-- <div class="is-size-5 publication-authors">
                <span class="author-block"
                  ><sup style="color: #6fbf73">1</sup>ByteDance,</span
                ><br />
                <span class="author-block"
                  ><sup style="color: #ed4b82">2</sup>Institute of Automation,
                  Chinese Academy of Sciences,</span
                >
                <span class="author-block"
                  ><sup style="color: #ffac33">3</sup>University of Texas at
                  Austin</span
                >
              </div> -->


              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- Arxiv PDF link -->
                  <!-- <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/2311.11567"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a
                      href="https://github.com/InfiMM/InfiMM-Eval-Tool"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                    <!-- <span class="link-block">
                      <a
                        href="https://huggingface.co/datasets/Infi-MM/InfiMM-Eval"
                        target="_blank"
                        class="external-link button is-normal is-rounded is-dark"
                      >
                        <span class="icon">
                          <img
                            src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg"
                            width="15px"
                            height="15px"
                          />
                        </span>
                        <span>Dataset</span>
                      </a>
                    </span> -->

                  <!-- Papers with Code link -->
                  <!-- <span class="link-block">
                    <a
                      href="https://paperswithcode.com/sota/visual-question-answering-vqa-on-core-mm"
                      target="_blank"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <img
                          src="https://info.arxiv.org/labs/images/pwc-logo.png"
                          width="15px"
                          height="15px"
                        />
                      </span>
                      <span>Leaderboard</span>
                    </a>
                  </span> -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <h2 class="subtitle has-text-centered">
            DeVAn is a novel human annotated dataset for evaluating the ability for visual-language models to generate both short and long descriptions for real-world video clips.
          </h2>
          <!-- <img src="images/compare_benchmark.png" alt="banner" width="100%" /> -->
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Overview</h2>
            <div class="content has-text-justified">
              <p>
                We present a novel human annotated dataset for evaluating the ability for visual-language models to generate both short and long descriptions for real-world video clips, termed <b>DeVAn</b> (Dense Video Annotation). The dataset contains 8.5K YouTube video clips of 20-60 seconds in duration and covers a wide range of topics and interests. Each video clip is independently annotated by 5 human annotators, producing both captions (1 sentence) and summaries (3-10 sentences). Given any video selected from the dataset and its corresponding ASR information, we evaluate visual-language models on either caption or summary generation that is grounded in both the visual and auditory content of the video. Additionally, models are also evaluated on caption- and summary-based retrieval tasks, where the summary-based retrieval task requires the identification of a target video given <i>excerpts</i> of a given summary. Given the novel nature of the paragraph-length video summarization task, we compared different existing evaluation metrics and their alignment with human preferences and found that model-based evaluation metrics provide more semantically-oriented and human-aligned evaluation. Finally, we benchmarked a wide range of current video-language models on DeVAn, and we aim for DeVAn to serve as a useful evaluation set in the age of large language models and complex multi-modal tasks.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Statistics</h2>
            <div class="content has-text-justified">
              <p>
                DeVAn is a multi-modal dataset containing 8.5K video clips carefully selected from 
                previously published YouTube-based video datasets (YouTube-8M and YT-Temporal-1B) 
                that integrate visual and auditory information. Over the span of 10 months, 
                a team of 24 human annotators (college and graduate level students) created 5 short 
                captions (1 sentence each) and 5 long summaries (3-10  sentences) for each video clip, 
                resulting in a rich and comprehensive human-annotated dataset that serves as a robust 
                ground truth for subsequent model training and evaluation.
              </p>
            </div>
          </div>
        </div>
        <div class="content has-text-centered">
          <img
            src="assets/data_diversity.pdf"
            alt="dataset statistics"
            width="80%"
          />
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3">Examples</h2>

          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="assets/examples/example0.png" alt="" width="100%" />
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="assets/examples/example0.png" alt="" width="100%" />
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="column is-full has-text-centered content">
          <h2 class="title is-3">Sample Predictions</h2>
          <p>
            Sampled predictions of models on DeVAn.
          </p>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="images/sample_prediction1.png" alt="" width="100%" />
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="images/sample_prediction2.png" alt="" width="100%" />
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="images/sample_prediction3.png" alt="" width="100%" />
              </div>
            </div>
            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="images/sample_prediction4.png" alt="" width="100%" />
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Leaderboard</h2>
            <div class="content has-text-justified">
              <table class="display" id="mainTable">
              </table>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Evaluation</h2>
            <div class="content has-text-justified">
              <p>
                To evaluate on our DeVAn Benchmark, please refer to description in our github <a href="https://github.com/TK-21st/DeVAn/README.md">README</a> file. 
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!--BibTex citation -->
    <!-- <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@misc{han2023coremm,
  title={InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models}, 
  author={Xiaotian Han and Quanzeng You and Yongfei Liu and Wentao Chen and Huangjie Zheng and Khalil Mrini and Xudong Lin and Yiqi Wang and Bohan Zhai and Jianbo Yuan and Heng Wang and Hongxia Yang},
  year={2023},
  eprint={2311.11567},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}</code></pre>
      </div>
    </section> -->
    <!--End BibTex citation -->

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                <!-- Please feel free to contact us via email @<a
                  href="mailto:infimmbytedance@gmail.com"
                  >infimmbytedance@gmail.com</a
                >
                if you have any questions. -->
                Under double-blind anonymous review.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

    <script>
      // $(document).ready( function () {
      //   $('.mainTable').DataTable();
      // } );
      new DataTable("#mainTable", {
        paging: false,
        order: [[8, "dsc"]],
      });
    </script>
  </body>
</html>